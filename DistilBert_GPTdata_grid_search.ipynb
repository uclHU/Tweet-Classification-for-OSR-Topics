{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h_6liWTWUbX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate\n",
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II6GlB16W_AS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "from datasets import Dataset\n",
        "import datasets\n",
        "\n",
        "df = pd.read_csv(\"./gpt_tweets_without_T_U_U.csv\", engine='python')\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/gpt_tweets_without_T_U_U.csv\", engine='python')\n",
        "\n",
        "#remove redundant records\n",
        "redundant_topic = ['Ukraine']\n",
        "df = df[~df['topic'].isin(redundant_topic)]\n",
        "\n",
        "#convert to list\n",
        "docs = df.text\n",
        "\n",
        "labels = df.topic\n",
        "\n",
        "id2label = {}\n",
        "label2id = {}\n",
        "id_counter = 0\n",
        "for i in range(len(labels)):\n",
        "    label = labels.iloc[i]\n",
        "    if label not in label2id:\n",
        "        label2id[label] = id_counter\n",
        "        id_counter += 1\n",
        "\n",
        "for label, id in label2id.items():\n",
        "    id2label[id] = label\n",
        "\n",
        "for i in range(len(labels)):\n",
        "    topic = labels.iloc[i]\n",
        "    cur_id = label2id[topic]\n",
        "    labels.iloc[i] = cur_id\n",
        "\n",
        "# generate class weight list\n",
        "id2counter = dict(id2label)\n",
        "label2counter = dict(label2id)\n",
        "\n",
        "for id, _ in id2counter.items():\n",
        "    id2counter[id] = 0\n",
        "    label = id2label[id]\n",
        "    label2counter[label] = 0\n",
        "\n",
        "for i in range(len(labels)):\n",
        "    cur_id = labels.iloc[i]\n",
        "    id2counter[cur_id] += 1\n",
        "    cur_topic = id2label[cur_id]\n",
        "    label2counter[cur_topic] += 1\n",
        "\n",
        "for label, counter in label2counter.items():\n",
        "    label2counter[label] = counter\n",
        "    # label2counter[label] = counter/len(labels)\n",
        "\n",
        "class_weight = []\n",
        "id_counter = 0\n",
        "for id_num, counter in id2counter.items():\n",
        "    weight = 1/(counter/len(df))\n",
        "    class_weight.append(weight)\n",
        "    id_counter += 1\n",
        "\n",
        "docs = docs.astype(str)\n",
        "df = pd.concat([docs, labels], axis=1)\n",
        "df.rename(columns={'topic':'label'}, inplace = True)\n",
        "\n",
        "dataset = ds.dataset(pa.Table.from_pandas(df).to_batches())\n",
        "\n",
        "### convert to Huggingface dataset\n",
        "hg_dataset = Dataset(pa.Table.from_pandas(df))\n",
        "\n",
        "train_dataset, test_dataset= hg_dataset.train_test_split(test_size=0.2, shuffle=True, seed=10).values()\n",
        "db = datasets.DatasetDict({\"train\":train_dataset,\"test\":test_dataset})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8OdmAHLW_FU"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmzn4kpiW_H0"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "import evaluate\n",
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        # forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "        # compute custom loss\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weight, device=model.device))\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_db = db.map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Performance measure metrics\n",
        "def custom_metrics(eval_pred):\n",
        "    metric1 = evaluate.load(\"precision\")\n",
        "    metric2 = evaluate.load(\"recall\")\n",
        "    metric3 = evaluate.load(\"f1\")\n",
        "    metric4 = evaluate.load(\"accuracy\")\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "    precision = metric1.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n",
        "    recall = metric2.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n",
        "    f1 = metric3.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy}\n",
        "\n",
        "batch_size_list = [16, 32]\n",
        "lr_list = [5e-5, 3e-5, 2e-5]\n",
        "num_epochs = 10\n",
        "\n",
        "for batch_size in batch_size_list:\n",
        "    for lr in lr_list:\n",
        "        # training using PyTorch\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"distilbert-base-uncased\", num_labels=id_counter, id2label=id2label, label2id=label2id\n",
        "        )\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"distilBERT_gptdata_with_preprocessing_grid_search\",\n",
        "            learning_rate=lr,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            num_train_epochs=10,\n",
        "            weight_decay=0.01,\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            push_to_hub=True,\\\n",
        "        )\n",
        "\n",
        "        trainer = CustomTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_db[\"train\"],\n",
        "            eval_dataset=tokenized_db[\"test\"],\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=data_collator,\n",
        "            compute_metrics=custom_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        trainer.push_to_hub()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}