{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7360f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testsets\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# df = pd.read_csv(\"./osr_tweets_without_T_U_U_v2.csv\", engine='python')\n",
    "df = pd.read_csv(\"./gpt_tweets_without_T_U_U.csv\", engine='python')\n",
    "\n",
    "#convert to list\n",
    "docs = df.text\n",
    "\n",
    "labels = df.topic\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "id_counter = 0\n",
    "for i in range(len(labels)):\n",
    "    label = labels.iloc[i]\n",
    "    if label not in label2id:\n",
    "        label2id[label] = id_counter\n",
    "        id_counter += 1\n",
    "\n",
    "for label, id in label2id.items():\n",
    "    id2label[id] = label\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    topic = labels.iloc[i]\n",
    "    cur_id = label2id[topic]\n",
    "    labels.iloc[i] = cur_id\n",
    "\n",
    "# generate class weight list\n",
    "id2counter = dict(id2label)\n",
    "label2counter = dict(label2id)\n",
    "\n",
    "for id, _ in id2counter.items():\n",
    "    id2counter[id] = 0\n",
    "    label = id2label[id]\n",
    "    label2counter[label] = 0\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    cur_id = labels.iloc[i]\n",
    "    id2counter[cur_id] += 1\n",
    "    cur_topic = id2label[cur_id]\n",
    "    label2counter[cur_topic] += 1\n",
    "\n",
    "for label, counter in label2counter.items():\n",
    "    label2counter[label] = counter\n",
    "    # label2counter[label] = counter/len(labels)\n",
    "\n",
    "class_weight = []\n",
    "id_counter = 0\n",
    "for id_num, counter in id2counter.items():\n",
    "    weight = 1/(counter/len(df))\n",
    "    class_weight.append(weight)\n",
    "    id_counter += 1\n",
    "\n",
    "docs = docs.astype(str)\n",
    "df = pd.concat([docs, labels], axis=1)\n",
    "df.rename(columns={'topic':'label'}, inplace = True)\n",
    "\n",
    "dataset = ds.dataset(pa.Table.from_pandas(df).to_batches())\n",
    "\n",
    "### convert to Huggingface dataset\n",
    "hg_dataset = Dataset(pa.Table.from_pandas(df))\n",
    "\n",
    "train_dataset, test_dataset= hg_dataset.train_test_split(test_size=0.2, shuffle=True, seed=10).values()\n",
    "db = datasets.DatasetDict({\"train\":train_dataset,\"test\":test_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weight, device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_db = db.map(preprocess_function, batched=True)\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Performance measure metrics\n",
    "def custom_metrics(eval_pred):\n",
    "    metric1 = evaluate.load(\"precision\")\n",
    "    metric2 = evaluate.load(\"recall\")\n",
    "    metric3 = evaluate.load(\"f1\")\n",
    "    metric4 = evaluate.load(\"accuracy\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    precision = metric1.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n",
    "    recall = metric2.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n",
    "    f1 = metric3.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aaf460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "test_training_args = TrainingArguments(\"test_trainer\")\n",
    "\n",
    "# gpt_model = AutoModelForSequenceClassification.from_pretrained(\"LovenOO/distilBERT_gptdata_with_preprocessing_grid_search\")\n",
    "\n",
    "gpt_model = AutoModelForSequenceClassification.from_pretrained(\"LovenOO/distilBERT_gptdata_with_preprocessing_grid_search\",revision='76a8e24648f9360c180b80b3cf176e2d6ada5c8a')\n",
    "\n",
    "# distilbert_model = AutoModelForSequenceClassification.from_pretrained(\"LovenOO/distilBERT_with_preprocessing_grid_search\", revision='e8460a5b00cb63ec52c235227b45a1fabf1f2056')\n",
    "\n",
    "# distilbert_model_without_hashtag = AutoModelForSequenceClassification.from_pretrained(\"LovenOO/distilBERT_with_preprocessing_grid_search\", revision='d8b7b2dd48cccecc48036255fb440b3b1d8ddff8')\n",
    "\n",
    "# bert_model_without_hashtag = AutoModelForSequenceClassification.from_pretrained(\"LovenOO/BERT_with_preprocessing_grid_search\", revision='1f5ba5fd1ece623860f24b0b5ecb5c5f3a4c9396')\n",
    "\n",
    "# bert_large_model_without_hashtag = AutoModelForSequenceClassification.from_pretrained(\"LovenOO/BERT_large_with_preprocessing_grid_search\", revision='dac309e589385dabbe9dab48693a351334620fc0')\n",
    "\n",
    "# distilbert_model_with_hashtag = AutoModelForSequenceClassification.from_pretrained(\"LovenOO/distilBERT_without_preprocessing_grid_search\", revision='ee83c5678b68ea6942e31508ccd286fc993f0f3d')\n",
    "\n",
    "# bert_model_with_hashtag = AutoModelForSequenceClassification.from_pretrained(\"LovenOO/BERT_without_preprocessing_grid_search\", revision='49e9c91823bd49e038747b14bdf07ec2dc9a3942')\n",
    "\n",
    "# bert_large_model_with_hashtag = AutoModelForSequenceClassification.from_pretrained(\"LovenOO/BERT_large_without_preprocessing\", revision='ba78578c0fb89f84f83e1be748c258d64c1ef52b')\n",
    "\n",
    "test_trainer = Trainer(\n",
    "    model=gpt_model,\n",
    "    args=test_training_args,\n",
    "    train_dataset=[],\n",
    "    eval_dataset=tokenized_db['test'],\n",
    "    compute_metrics=custom_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "test_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95121864",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on ensemble model of DistilBert, BERT-base, BERT-large, DistilBert on GPT set\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/test_set.csv\", engine='python')\n",
    "\n",
    "gpt = pipeline(model=\"LovenOO/distilBERT_gptdata_with_preprocessing_grid_search\",revision='76a8e24648f9360c180b80b3cf176e2d6ada5c8a')\n",
    "\n",
    "distilbert_model_without_hashtag = pipeline(model=\"LovenOO/distilBERT_with_preprocessing_grid_search\", revision='d8b7b2dd48cccecc48036255fb440b3b1d8ddff8')\n",
    "\n",
    "bert_model_without_hashtag = pipeline(model=\"LovenOO/BERT_with_preprocessing_grid_search\", revision='1f5ba5fd1ece623860f24b0b5ecb5c5f3a4c9396')\n",
    "\n",
    "bert_large_model_without_hashtag = pipeline(model=\"LovenOO/BERT_large_with_preprocessing_grid_search\", revision='dac309e589385dabbe9dab48693a351334620fc0')\n",
    "\n",
    "predicted_list = []\n",
    "\n",
    "for text in tqdm(df.text, total=df.shape[0]):\n",
    "    cur_list = []\n",
    "    cur_list.append(gpt(text)[0]['label'])\n",
    "    cur_list.append(distilbert_model_without_hashtag(text)[0]['label'])\n",
    "    cur_list.append(bert_model_without_hashtag(text)[0]['label'])\n",
    "    cur_list.append(bert_large_model_without_hashtag(text)[0]['label'])\n",
    "\n",
    "    sorted_idx = np.argsort(np.array(cur_list))\n",
    "    sorted_list = np.array(cur_list)[sorted_idx]\n",
    "    if sorted_list[0]==sorted_list[1] and sorted_list[2]==sorted_list[3]:\n",
    "        predicted_list.append(cur_list[3])\n",
    "    else:\n",
    "        id = np.argmax(np.array(sorted_list))\n",
    "        predicted_list.append(sorted_list[id])\n",
    "\n",
    "# normalize text label\n",
    "temp_list = []\n",
    "for pred_label in predicted_list:\n",
    "    temp_list.append(pred_label.replace(',',''))\n",
    "\n",
    "id2label = {\n",
    "0 : 'Children Education and Skills',\n",
    "1 : 'Health and Social Care',\n",
    "2 : 'Crime and Security' ,\n",
    "3 : 'Economy',\n",
    "4 : 'Housing Planning and Local Services',\n",
    "5 : 'Labour Market and Welfare' ,\n",
    "6 : 'Population and Society' ,\n",
    "7 : 'Transport Environment and Climate Change'\n",
    "}\n",
    "\n",
    "# convert text label to id label\n",
    "label2id = {}\n",
    "for id, label in id2label.items():\n",
    "    label2id[label] = id\n",
    "predicted_id_list = []\n",
    "for pred_label in temp_list:\n",
    "    cur_id = label2id[pred_label]\n",
    "    predicted_id_list.append(cur_id)\n",
    "\n",
    "print(\" Accuracy:\", np.mean(np.array(predicted_id_list) == df.label.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c74320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on ensemble model of DistilBert, BERT-base, BERT-large\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/test_set.csv\", engine='python')\n",
    "\n",
    "\n",
    "distilbert_model_without_hashtag = pipeline(model=\"LovenOO/distilBERT_with_preprocessing_grid_search\", revision='d8b7b2dd48cccecc48036255fb440b3b1d8ddff8')\n",
    "\n",
    "bert_model_without_hashtag = pipeline(model=\"LovenOO/BERT_with_preprocessing_grid_search\", revision='1f5ba5fd1ece623860f24b0b5ecb5c5f3a4c9396')\n",
    "\n",
    "bert_large_model_without_hashtag = pipeline(model=\"LovenOO/BERT_large_with_preprocessing_grid_search\", revision='dac309e589385dabbe9dab48693a351334620fc0')\n",
    "\n",
    "predicted_list = []\n",
    "for text in df.text:\n",
    "    cur_list = []\n",
    "    cur_list.append(distilbert_model_without_hashtag(text)[0]['label'])\n",
    "    cur_list.append(bert_model_without_hashtag(text)[0]['label'])\n",
    "    cur_list.append(bert_large_model_without_hashtag(text)[0]['label'])\n",
    "\n",
    "    sorted_idx = np.argsort(np.array(cur_list))\n",
    "    sorted_list = np.array(cur_list)[sorted_idx]\n",
    "    id = np.argmax(np.array(sorted_list))\n",
    "    predicted_list.append(sorted_list[id])\n",
    "\n",
    "temp_list = []\n",
    "for pred_label in predicted_list:\n",
    "    temp_list.append(pred_label.replace(',',''))\n",
    "\n",
    "id2label = {\n",
    "0 : 'Children Education and Skills',\n",
    "1 : 'Health and Social Care',\n",
    "2 : 'Crime and Security' ,\n",
    "3 : 'Economy',\n",
    "4 : 'Housing Planning and Local Services',\n",
    "5 : 'Labour Market and Welfare' ,\n",
    "6 : 'Population and Society' ,\n",
    "7 : 'Transport Environment and Climate Change'\n",
    "}\n",
    "label2id = {}\n",
    "for id, label in id2label.items():\n",
    "    label2id[label] = id\n",
    "predicted_id_list = []\n",
    "for pred_label in temp_list:\n",
    "    cur_id = label2id[pred_label]\n",
    "    predicted_id_list.append(cur_id)\n",
    "\n",
    "print(\" Accuracy:\", np.mean(np.array(predicted_id_list) == df.label.values))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
